---
title: "GE_CustomerChurnModel"
author: "Clay Gendron"
date: "11/14/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

## Library Setup
  
```{r Establish Library}
library(tidymodels)
library(tidyverse)
library(here)
library(corrplot)
library(randomForest)
library(magrittr)
library(xgboost)
library(ggplot2)
library(rsample)
library(glmnet)
library(coefplot)
library(skimr)
library(recipes)
library(dygraphs)
library(lattice)
library(caret)
library(rattle)
```

## Data Pull and Initial Split

```{r Data}

# data pull
churn_dataset <- read.csv(here::here('Data/ge_cell_data.csv'))

# cal and val tables
set.seed(1379) # for reproduction
churn_split <- initial_split(churn_dataset, prop = .8, strata = CHURN) # create calibration and validation datasets

churn_calibration <- training(churn_split) # train
churn_validation <- testing(churn_split) # test

```

## Explorting Data

```{r EDA}

## Variable Selection for EDA Visuals

# output column names and summarise data
names(churn_dataset)
skim(churn_dataset)

# eliminate non usefull variables
var_mod1 <- glm(
  CHURN ~ . - CHURNDEP - CALIBRAT - CSA - OCC_LABEL, data=churn_dataset, family = binomial
)
summary(var_mod1) # identify roughly significant variables to explore

# eliminate non important predictors variables - keep top 20 predictors
var_mod2 <- lm(
  CHURN ~ 
    MOU + REFURB + OCCCLER + WEBCAP + MODELS + 
    ROAM + OVERAGE + PRIZMUB + PRZM_NUM + CHANGER + 
    MOUREC + PHONES + MAILORD + CHANGEM + PRIZMRUR + 
    SETPRC + RETCALLS + CREDITGY + BLCKVCE + DROPBLK, 
  data=churn_calibration, family = binomial
)
summary(var_mod2)

coefplot(var_mod2, sort = "magnitude") # rough understanding of correlation to CHURN

## Data Visualizations

# histograms and bars

ggplot(churn_calibration, aes(x = MOU)) + geom_histogram()
ggplot(churn_calibration, aes(x = REFURB)) + geom_bar()
ggplot(churn_calibration, aes(x = OCCCLER)) + geom_bar()
ggplot(churn_calibration, aes(x = WEBCAP)) + geom_bar()
ggplot(churn_calibration, aes(x = MODELS)) + geom_bar()
ggplot(churn_calibration, aes(x = ROAM)) + geom_histogram()
ggplot(churn_calibration, aes(x = OVERAGE)) + geom_histogram()
ggplot(churn_calibration, aes(x = PRIZMUB)) + geom_bar()
ggplot(churn_calibration, aes(x = PRZM_NUM)) + geom_bar()
ggplot(churn_calibration, aes(x = CHANGER)) + geom_histogram()
ggplot(churn_calibration, aes(x = MOUREC)) + geom_histogram()
ggplot(churn_calibration, aes(x = PHONES)) + geom_bar()
ggplot(churn_calibration, aes(x = MAILORD)) + geom_bar()
ggplot(churn_calibration, aes(x = CHANGEM)) + geom_histogram()
ggplot(churn_calibration, aes(x = PRIZMRUR)) + geom_bar()
ggplot(churn_calibration, aes(x = SETPRC)) + geom_bar()
ggplot(churn_calibration, aes(x = RETCALLS)) + geom_bar()
ggplot(churn_calibration, aes(x = CREDITGY)) + geom_bar()
ggplot(churn_calibration, aes(x = BLCKVCE)) + geom_histogram()
ggplot(churn_calibration, aes(x = DROPBLK)) + geom_histogram()

```

## Recipes and Data Manipulations

```{r Recipes and Data Manipulations}
## Keep Relevent Columns Create Test and Train Datasets

train <- churn_calibration %>% select(CUSTOMER, MOU, REFURB, OCCCLER, WEBCAP, MODELS, ROAM, OVERAGE, PRIZMUB, PRZM_NUM, CHANGER, MOUREC, PHONES, MAILORD, CHANGEM, PRIZMRUR, SETPRC, RETCALLS, CREDITGY, BLCKVCE, DROPBLK, CHURN)
test <- churn_validation %>% select(CUSTOMER, MOU, REFURB, OCCCLER, WEBCAP, MODELS, ROAM, OVERAGE, PRIZMUB, PRZM_NUM, CHANGER, MOUREC, PHONES, MAILORD, CHANGEM, PRIZMRUR, SETPRC, RETCALLS, CREDITGY, BLCKVCE, DROPBLK, CHURN)

## Recipes and Data Manipulations

# recipes

m_rec1 <- recipe(CHURN ~ ., data = train) %>% 
  step_rm(CUSTOMER) %>% # remove customer ID
  step_knnimpute(all_predictors()) %>% # impute any missing variables for modeling purposes
  step_nzv(all_predictors()) %>% # remove variables that are highly spare or unbalanced
  step_normalize(all_numeric(), - CHURN) %>% # mean of 0 and sd of 1 for all numeric variables
  step_BoxCox(all_numeric(), - CHURN) # simple Box-Cox transformation

m_prep1 <- prep(m_rec1, training = train)

# bake

m_train <- bake(m_prep1, new_data = train)
m_test <- bake(m_prep1, new_data = test)
m_test_id <- cbind(test$CUSTOMER, m_test) %>% rename("CUSTOMER" = "test$CUSTOMER")

# build matrix for XG Boost

m_train_x <- bake(m_prep1, new_data = m_train, all_predictors(), composition = "matrix")
m_train_y <- bake(m_prep1, new_data = m_train, all_outcomes(), composition = "matrix")
m_train_xg <- xgb.DMatrix(data = m_train_x, label = m_train_y)

m_test_x <- bake(m_prep1, new_data = m_test, all_predictors(), composition = "matrix")
m_test_y <- bake(m_prep1, new_data = m_test, all_outcomes(), composition = "matrix")
m_test_xg <- xgb.DMatrix(data = m_test_x, label = m_test_y)

```

## Build Prediction Model - Tree XG Boost

```{r Build Prediction Model - Tree XG Boost}

# XG Boost Model

xgb_churn_mod <- xgb.train(
  data = m_train_xg,
  max_depth = 4,
  lambda = 1,
  nrounds = 10,
  objective = 'binary:logistic',
  booster = 'gbtree',
  watchlist = list(train = m_train_xg, validate = m_test_xg),
  print_every_n = 1
)

xgb.plot.multi.trees(xgb_churn_mod) # plot the tree
dygraph(xgb_churn_mod$evaluation_log) # validate test and train
xgb.importance(model = xgb_churn_mod) # identify important predictors

CHURN_Rank_XGB <- predict(xgb_churn_mod,m_test_xg) # predict churn on test
CHURN_P_XGB <- as.numeric(CHURN_Rank_XGB > 0.30) # make prediction binary for cf matrix
val_table <- cbind(m_test_id,CHURN_P_XGB,CHURN_Rank_XGB) # join to single table

CHURN_Rank_Train <- predict(xgb_churn_mod,m_train_xg) # predict churn for test
CHURN_P_Train <- as.numeric(CHURN_Rank_Train > 0.30) # binary prediction for test
val_table_train <- cbind(m_train,CHURN_P_Train,CHURN_Rank_Train) # join to single table

```

## Evaluating Model Performance - Tree XG Boost

```{r}

# Confusion Matrix

# test
cf_churn <- table(val_table$CHURN,val_table$CHURN_P_XGB)
caret::confusionMatrix(cf_churn)

# train
cf_churn_train <- table(val_table_train$CHURN,val_table_train$CHURN_P_Train)
caret::confusionMatrix(cf_churn_train)

# Risk Chart

xgb_riskchart <- riskchart(CHURN_Rank_XGB,
                    val_table$CHURN, 
                    title="Customer Outreach", 
                    recall.name="Churn Prediction", precision.name = "Strike Rate",
                    show.lift=TRUE, show.precision=TRUE, legend.horiz=FALSE, show.maximal = TRUE)
print(xgb_riskchart) # chart to understand how the predictions impact operational caseloads

```

## Build Prediction Model - K Means Clustering

```{r}

# Build Modeling Tables

k_train <- m_train %>% dplyr::select(CHANGEM, MOU, CHANGER, DROPBLK, MOUREC, REFURB, OVERAGE, ROAM, PRIZMUB, SETPRC, MAILORD, BLCKVCE, MODELS, PRIZMRUR) %>% na.omit()
k_test <- m_test_id %>% dplyr::select(CUSTOMER, CHANGEM, MOU, CHANGER, DROPBLK, MOUREC, REFURB, OVERAGE, ROAM, PRIZMUB, SETPRC, MAILORD, BLCKVCE, MODELS, PRIZMRUR) %>% na.omit()

# Create K Means Model
set.seed(9731) # for reporduction
k_mod <- kmeans(k_train,centers = 10, nstart = 500) 
k_mod
CUST_Clusters_Train <- k_mod$cluster # cluster assignement outputs
cluster_train <- cbind(m_train, CUST_Clusters_Train) # training dataset with clusters

# Create Cluster Churn Probability Table

ge_cust_clusters1 <- aggregate.data.frame(cluster_train$CHURN, 
                          by = list(CUST_Clusters_Train),
                          FUN = mean) # table to have CHURN probabilities by cluster
colnames(ge_cust_clusters1)

ge_cust_clusters2 <- ge_cust_clusters1 %>% 
  rename("CUST_Cluster" = "Group.1", "CHURN_Rank_KM" = "x")

ge_cust_clusters3 <- ge_cust_clusters2[order(ge_cust_clusters2$CHURN_Rank_KM),]

cluster_rank <- data.frame(c(10:1)) # for reordering cluster numbers to make logical sense
colnames(cluster_rank)
cluster_rank <- cluster_rank %>% rename("Customer_Group" = "c.10.1.")

ge_cust_clusters <- cbind(ge_cust_clusters3,cluster_rank) # cluster groups table

```

## Add Cluster Probabilities to XG Boost Probabilities

```{r Final Analytic Model}
# Create Aggregated Prediction

CUST_Cluster <- predict(k_mod, val_table) # predict cluster or test data

cluster_table <- cbind(val_table, CUST_Cluster)
cluster_table2 <- merge(x = cluster_table, y = ge_cust_clusters, by.x = "CUST_Cluster", by.y = "CUST_Cluster") # add cluster CHURN probabilities
CHURN_Rank <- (cluster_table2$CHURN_Rank_XGB + cluster_table2$CHURN_Rank_KM) / 2 # average XGB probabilities with KM cluster probabilities
CHURN_P <- as.numeric(CHURN_Rank > 0.30) # make aggregated prediction binary
cluster_table3 <- cbind(cluster_table2,CHURN_P,CHURN_Rank) 
colnames(cluster_table3)
cluster_table4 <- cluster_table3 %>% select(CUSTOMER, CHURN, CHURN_Rank_XGB, CHURN_Rank_KM, CHURN_Rank, CHURN_P, Customer_Group)

# exportable table
test_minus_CHURN <- select(churn_validation, -c("CHURN"))
ge_cust_churn_df <- merge(x = test_minus_CHURN, y = cluster_table4, by.x = "CUSTOMER", by.y = "CUSTOMER")
summary(ge_cust_churn_df)

```

# Logistic Regression

```{r}

log_churn_mod <- glm(
  CHURN ~ CHANGEM + MOU + CHANGER + DROPBLK + MOUREC + REFURB + OVERAGE + ROAM + PRIZMUB + SETPRC + MAILORD + BLCKVCE + MODELS + PRIZMRUR, 
  data = m_train, 
  family = binomial("logit")
)

summary(log_churn_mod)

CHURN_Rank_log <- predict(log_churn_mod, m_test, type = "response")
CHURN_P_log <- as.numeric(CHURN_Rank_log > 0.30)
log_p_df <- cbind(m_test_id,CHURN_Rank_log,CHURN_P_log)
log_df_merge <- log_p_df %>% select(CUSTOMER,CHURN_Rank_log,CHURN_P_log)

ge_cust_churn_df <- merge(x = ge_cust_churn_df, y = log_df_merge, by.x = "CUSTOMER", by.y = "CUSTOMER")

```


# Model Evaluation

```{r Final Model Assessment}
# Export Dataset

write.csv(ge_cust_churn_df,here::here('Data/ge_cust_churn_predictions.csv'))

# Confusion Matrix

cf_ge_churn <- table(ge_cust_churn_df$CHURN,ge_cust_churn_df$CHURN_P)
caret::confusionMatrix(cf_ge_churn)

cf_ge_churn_log <- table(ge_cust_churn_df$CHURN,ge_cust_churn_df$CHURN_P_log)
caret::confusionMatrix(cf_ge_churn_log)

# Caseload Analysis

ge_riskchart <- riskchart(ge_cust_churn_df$CHURN_Rank,
                    ge_cust_churn_df$CHURN, 
                    title="Customer Outreach", 
                    recall.name="Churn Prediction", precision.name = "Strike Rate",
                    show.lift=TRUE, show.precision=TRUE, legend.horiz=FALSE, show.maximal = TRUE)
print(ge_riskchart) # chart to understand how the predictions impact operational caseloads

ge_riskchart_log <- riskchart(ge_cust_churn_df$CHURN_Rank_log,
                    ge_cust_churn_df$CHURN, 
                    title="Customer Outreach", 
                    recall.name="Churn Prediction", precision.name = "Strike Rate",
                    show.lift=TRUE, show.precision=TRUE, legend.horiz=FALSE, show.maximal = TRUE)
print(ge_riskchart_log) # chart to understand how the predictions impact operational caseloads

```


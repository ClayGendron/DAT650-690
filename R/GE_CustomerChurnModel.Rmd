---
title: "GE_CustomerChurnModel"
author: "Clay Gendron"
date: "11/14/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

## Library Setup
  
```{r Establish Library}
library(tidymodels)
library(tidyverse)
library(here)
library(corrplot)
library(randomForest)
library(magrittr)
library(xgboost)
library(ggplot2)
library(rsample)
library(glmnet)
library(coefplot)
library(skimr)
library(recipes)
library(dygraphs)
library(lattice)
library(caret)
library(rattle)
```

## Data Pull and Split

```{r Data}

# data pull
churn_dataset <- read.csv(here::here('Data/ge_cell_data.csv'))

# cal and val tables
set.seed(1379)
churn_split <- initial_split(churn_dataset, prop = .8, strata = CHURN)

churn_calibration <- training(churn_split)
churn_validation <- testing(churn_split)

```

## Explorting Data

```{r EDA}

## Variable Selection for EDA Visuals

# output column names and summarise data
names(churn_dataset)
skim(churn_dataset)

# eliminate non usefull variables
var_mod1 <- glm(
  CHURN ~ . - CHURNDEP - CALIBRAT - CSA - OCC_LABEL, data=churn_dataset, family = binomial
)
summary(var_mod1)

# eliminate non important predictors variables - keep top 20 predictors
var_mod2 <- lm(
  CHURN ~ 
    MOU + REFURB + OCCCLER + WEBCAP + MODELS + 
    ROAM + OVERAGE + PRIZMUB + PRZM_NUM + CHANGER + 
    MOUREC + PHONES + MAILORD + CHANGEM + PRIZMRUR + 
    SETPRC + RETCALLS + CREDITGY + BLCKVCE + DROPBLK, 
  data=churn_calibration, family = binomial
)
summary(var_mod2)

coefplot(var_mod2, sort = "magnitude")

## Data Visualizations

# histograms and bars

ggplot(churn_calibration, aes(x = MOU)) + geom_histogram()
ggplot(churn_calibration, aes(x = REFURB)) + geom_bar()
ggplot(churn_calibration, aes(x = OCCCLER)) + geom_bar()
ggplot(churn_calibration, aes(x = WEBCAP)) + geom_bar()
ggplot(churn_calibration, aes(x = MODELS)) + geom_bar()
ggplot(churn_calibration, aes(x = ROAM)) + geom_histogram()
ggplot(churn_calibration, aes(x = OVERAGE)) + geom_histogram()
ggplot(churn_calibration, aes(x = PRIZMUB)) + geom_bar()
ggplot(churn_calibration, aes(x = PRZM_NUM)) + geom_bar()
ggplot(churn_calibration, aes(x = CHANGER)) + geom_histogram()
ggplot(churn_calibration, aes(x = MOUREC)) + geom_histogram()
ggplot(churn_calibration, aes(x = PHONES)) + geom_bar()
ggplot(churn_calibration, aes(x = MAILORD)) + geom_bar()
ggplot(churn_calibration, aes(x = CHANGEM)) + geom_histogram()
ggplot(churn_calibration, aes(x = PRIZMRUR)) + geom_bar()
ggplot(churn_calibration, aes(x = SETPRC)) + geom_bar()
ggplot(churn_calibration, aes(x = RETCALLS)) + geom_bar()
ggplot(churn_calibration, aes(x = CREDITGY)) + geom_bar()
ggplot(churn_calibration, aes(x = BLCKVCE)) + geom_histogram()
ggplot(churn_calibration, aes(x = DROPBLK)) + geom_histogram()

```

## Recipes and Data Manipulations

```{r Recipes and Data Manipulations}
## Keep Relevent Columns Create Test and Train Datasets

train <- churn_calibration %>% select(CUSTOMER, MOU, REFURB, OCCCLER, WEBCAP, MODELS, ROAM, OVERAGE, PRIZMUB, PRZM_NUM, CHANGER, MOUREC, PHONES, MAILORD, CHANGEM, PRIZMRUR, SETPRC, RETCALLS, CREDITGY, BLCKVCE, DROPBLK, CHURN)
test <- churn_validation %>% select(CUSTOMER, MOU, REFURB, OCCCLER, WEBCAP, MODELS, ROAM, OVERAGE, PRIZMUB, PRZM_NUM, CHANGER, MOUREC, PHONES, MAILORD, CHANGEM, PRIZMRUR, SETPRC, RETCALLS, CREDITGY, BLCKVCE, DROPBLK, CHURN)

## Recipes and Data Manipulations

# recipes

m_rec1 <- recipe(CHURN ~ ., data = train) %>% 
  step_rm(CUSTOMER) %>% 
  step_knnimpute(all_predictors()) %>% 
  step_nzv(all_predictors()) %>% 
  step_normalize(all_numeric(), - CHURN) %>% 
  step_BoxCox(all_numeric(), - CHURN)

m_prep1 <- prep(m_rec1, training = train)

# bake

m_train <- bake(m_prep1, new_data = train)
m_test <- bake(m_prep1, new_data = test)

# build matrix

m_train_x <- bake(m_prep1, new_data = m_train, all_predictors(), composition = "matrix")
m_train_y <- bake(m_prep1, new_data = m_train, all_outcomes(), composition = "matrix")
m_train_xg <- xgb.DMatrix(data = m_train_x, label = m_train_y)

m_test_x <- bake(m_prep1, new_data = m_test, all_predictors(), composition = "matrix")
m_test_y <- bake(m_prep1, new_data = m_test, all_outcomes(), composition = "matrix")
m_test_xg <- xgb.DMatrix(data = m_test_x, label = m_test_y)

```

## Build Prediction Model - Tree XG Boost

```{r Build Prediction Model - Tree XG Boost}

# XG Boost Model

xgb_churn_mod <- xgb.train(
  data = m_train_xg,
  max_depth = 4,
  nrounds = 10,
  objective = 'binary:logistic',
  booster = 'gbtree',
  watchlist = list(train = m_train_xg, validate = m_test_xg),
  print_every_n = 1
)

xgb.plot.multi.trees(xgb_churn_mod) # Plot the Tree
dygraph(xgb_churn_mod$evaluation_log) # Validate Test and Train
xgb.importance(model = xgb_churn_mod) # Identify Important Predictors

CHURN_Rank_XGB <- predict(xgb_churn_mod,m_test_xg) # Predict Churn
CHURN_P_XGB <- as.numeric(CHURN_Rank_XGB > 0.50) # Make Prediction Binary
val_table <- cbind(m_test,CHURN_P_XGB,CHURN_Rank_XGB) # Join to Test Table

CHURN_Rank_Train <- predict(xgb_churn_mod,m_train_xg) # Predict Churn
CHURN_P_Train <- as.numeric(CHURN_Rank_Train > 0.50) # Make Prediction Binary
val_table_train <- cbind(m_train,CHURN_P_Train,CHURN_Rank_Train) # Join to Train Table

```

## Evaluating Model Performance - Tree XG Boost

```{r}

# Confusion Matrix

# test
cf_churn <- table(val_table$CHURN,val_table$CHURN_P_XGB)
caret::confusionMatrix(cf_churn)

# train
cf_churn_train <- table(val_table_train$CHURN,val_table_train$CHURN_P_Train)
caret::confusionMatrix(cf_churn_train)

# Risk Chart

test_riskchart <- riskchart(CHURN_Rank_XGB,
                    val_table$CHURN, 
                    title="Customer Outreach", 
                    recall.name="Churn Prediction", precision.name = "Strick Rate",
                    show.lift=TRUE, show.precision=TRUE, legend.horiz=FALSE, show.maximal = TRUE)
print(test_riskchart)

```

## Build Prediction Model - K Means Clustering

```{r}

# Build Modeling Tables

k_train <- m_train %>% dplyr::select(CHURN, CHANGEM, MOU, CHANGER, DROPBLK, MOUREC) %>% na.omit()
k_test <- m_test %>% dplyr::select(CHURN, CHANGEM, MOU, CHANGER, DROPBLK, MOUREC) %>% na.omit()

# Create K Means Model
set.seed(9731)
k_mod <- kmeans(k_train,centers = 10, nstart = 500) 
k_mod
CUST_Clusters_Train <- k_mod$cluster
cluster_train <- cbind(k_train, CUST_Clusters_Train)

# Create Cluster Churn Probability Table

ge_cust_clusters1 <- aggregate.data.frame(cluster_train$CHURN, 
                          by = list(CUST_Clusters_Train),
                          FUN = mean)
colnames(ge_cust_clusters)

ge_cust_clusters2 <- ge_cust_clusters1 %>% 
  rename("CUST_Cluster" = "Group.1", "CHURN_Rank_KM" = "x")

ge_cust_clusters3 <- ge_cust_clusters2[order(ge_cust_clusters2$CHURN_Rank_KM),]

cluster_rank <- data.frame(c(10:1))
colnames(cluster_rank)
cluster_rank <- cluster_rank %>% rename("Customer_Group" = "c.10.1.")

ge_cust_clusters <- cbind(ge_cust_clusters3,cluster_rank)

# Add Cluster Probabilities to Validation Table

CUST_Cluster <- predict(k_mod, k_test)

cluster_table <- cbind(val_table, CUST_Cluster)
cluster_table2 <- merge(x = cluster_table, y = ge_cust_clusters, by.x = "CUST_Cluster", by.y = "CUST_Cluster")
CHURN_Rank <- (cluster_table2$CHURN_Rank_XGB + cluster_table2$CHURN_Rank_KM) / 2
CHURN_P <- as.numeric(CHURN_Rank > 0.50) # Make Prediction Binary
cluster_table3 <- cbind(cluster_table2,CHURN_P,CHURN_Rank)

cf_churn_final <- table(cluster_table3$CHURN,cluster_table3$CHURN_P)
caret::confusionMatrix(cf_churn_final)


test_riskchart <- riskchart(CHURN_Rank,
                    cluster_table3$CHURN, 
                    title="Customer Outreach", 
                    recall.name="Churn Prediction", precision.name = "Strick Rate",
                    show.lift=TRUE, show.precision=TRUE, legend.horiz=FALSE, show.maximal = TRUE)
print(test_riskchart)


cluster_table <- cbind(val_table, CUST_Cluster)
pva_clusters <- cbind(pva, k_mod$cluster)
```

